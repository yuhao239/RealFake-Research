{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import matplotlib.pyplot as plt\n",
    "import tqdm\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.base import BaseEstimator\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import accuracy_score\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "import regex as re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions\n",
    "def clean_text(text, remove_nums=True):\n",
    "    if remove_nums:\n",
    "        # Remove all numbers \n",
    "        result = re.sub(r'[0-9\\.]+', '', text)\n",
    "        # Lowercase and remove non-alphanumeric characters\n",
    "        return ''.join([char.lower() for char in result if char.isalnum() or char.isspace()])\n",
    "    else:\n",
    "        return ''.join([char.lower() for char in text if char.isalnum() or char.isspace()])\n",
    "\n",
    "def lemmatize_text(text, lemmatizer):\n",
    "    return ' '.join([lemmatizer.lemmatize(word) for word in text.split()])\n",
    "\n",
    "def preprocessing_texts(df, column_name, apply_lemmatization, lemmatizer):\n",
    "    texts = df[column_name].apply(clean_text)\n",
    "    if apply_lemmatization:\n",
    "        texts = texts.apply(lambda x: lemmatize_text(x, lemmatizer))\n",
    "    return texts\n",
    "\n",
    "def vectorize_datasets(train_df, test_df, vectorizer_type='count', ngram_range=(1, 1), remove_stopwords=False, apply_lemmatization=False):\n",
    "    lemmatizer = WordNetLemmatizer() if apply_lemmatization else None\n",
    "    stop_words = 'english' if remove_stopwords else None\n",
    "\n",
    "    # Choose Vectorizer: 'count' or 'tfidf'\n",
    "    if vectorizer_type == 'count':\n",
    "        vectorizer = CountVectorizer(stop_words=stop_words, ngram_range=ngram_range)\n",
    "    elif vectorizer_type == 'tfidf':\n",
    "        vectorizer = TfidfVectorizer(stop_words=stop_words, ngram_range=ngram_range)\n",
    "    else:\n",
    "        raise ValueError(\"Invalid vectorizer type\")\n",
    "\n",
    "    # Clean and lemmatize text\n",
    "    train_texts = preprocessing_texts(train_df, column_name='content', apply_lemmatization=apply_lemmatization, lemmatizer=lemmatizer)\n",
    "    test_texts = preprocessing_texts(test_df, column_name='content', apply_lemmatization=apply_lemmatization, lemmatizer=lemmatizer)\n",
    "\n",
    "    # Vectorize the text data\n",
    "    train_vectors = vectorizer.fit_transform(train_texts)\n",
    "    test_vectors = vectorizer.transform(test_texts)\n",
    "\n",
    "    # Extract the label column (real=1, fake=0)\n",
    "    y_train = train_df['label']\n",
    "    y_test = test_df['label']\n",
    "\n",
    "    return train_vectors, test_vectors, y_train, y_test, vectorizer\n",
    "\n",
    "def bleaching(textdf):\n",
    "    # Uses the clean_text() function to completely sanitize the text inside a pandas dataframe \n",
    "    for idx in textdf.index:\n",
    "        cleaned_text = clean_text(textdf.loc[idx, 'content'], remove_nums=True)\n",
    "        textdf.loc[idx, 'content'] = cleaned_text\n",
    "    return textdf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multinomial Likelihood Naive Bayes Model \n",
    "class NaiveBayes:\n",
    "    def __init__(self, laplace=1):\n",
    "        # Freely change the laplace factor when building the model\n",
    "        self.laplace = laplace\n",
    "\n",
    "    def fit(self, X, Y):\n",
    "        # Initialization\n",
    "        n_docs, n_features = X.shape\n",
    "        self.classes = np.unique(Y)\n",
    "        n_classes = len(self.classes)\n",
    "\n",
    "        # Prior and posterior parameters\n",
    "        self.class_log_prior = np.zeros(n_classes)\n",
    "        self.feature_log_post = np.zeros((n_classes, n_features))\n",
    "\n",
    "        # Learning process using Laplace prior and multinomial likelihood\n",
    "        for i, cls in enumerate(self.classes):\n",
    "            X_cls = X[Y == cls]\n",
    "            self.class_log_prior[i] = np.log(X_cls.shape[0] / n_docs)\n",
    "            total_count = np.sum(X_cls, axis=0) + self.laplace\n",
    "            # Possible Improvement\n",
    "            # P(word|class) = (Count(word, class) + laplace)/(Count(word) + Count(unique_word)*laplace) ## same\n",
    "            # self.feature_log_post[i, :] = np.log(total_count / (np.sum(total_count))\n",
    "            self.feature_log_post[i, :] = np.log(total_count / (np.sum(X_cls) + n_features * self.laplace))\n",
    "\n",
    "    def predict(self, X):\n",
    "        log_probs = X @ self.feature_log_post.T + self.class_log_prior\n",
    "        # Possible Improvement\n",
    "        # np.argmax(log_probs, axis=1)\n",
    "        return self.classes[np.argmax(log_probs, axis=1)]\n",
    "\n",
    "    def eval(self, X, Y):\n",
    "        y_pred = self.predict(X)\n",
    "        return np.mean(y_pred == Y)\n",
    "\n",
    "# Logistic Implementation\n",
    "class LogisticRegressionCustom(BaseEstimator):\n",
    "    def __init__(self, lr=0.01, n_iter=1000):\n",
    "        self.lr = lr\n",
    "        self.n_iter = n_iter\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        X = np.insert(X, 0, 1, axis=1)  # Add bias term\n",
    "        self.weights = np.zeros(X.shape[1])\n",
    "\n",
    "        for _ in range(self.n_iter):\n",
    "            linear_model = np.dot(X, self.weights)\n",
    "            y_pred = self.sigmoid(linear_model)\n",
    "            gradient = np.dot(X.T, (y_pred - y)) / len(y)\n",
    "            self.weights -= self.lr * gradient\n",
    "\n",
    "    def sigmoid(self, z):\n",
    "        return 1 / (1 + np.exp(-z))\n",
    "\n",
    "    def predict(self, X):\n",
    "        X = np.insert(X, 0, 1, axis=1)\n",
    "        linear_model = np.dot(X, self.weights)\n",
    "        y_pred = self.sigmoid(linear_model)\n",
    "        return [1 if i > 0.5 else 0 for i in y_pred]\n",
    "\n",
    "    def eval(self, X, Y):\n",
    "        y_pred = self.predict(X)\n",
    "        return accuracy_score(Y, y_pred)\n",
    "\n",
    "\n",
    "# SVM Implementation (Hinge Loss)\n",
    "class SVMCustom(BaseEstimator):\n",
    "    def __init__(self, lr=0.01, n_iter=1000, C=1.0):\n",
    "        self.lr = lr\n",
    "        self.n_iter = n_iter\n",
    "        self.C = C\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        y = np.where(y <= 0, -1, 1)\n",
    "        self.weights = np.zeros(X.shape[1])\n",
    "\n",
    "        for _ in range(self.n_iter):\n",
    "            for idx, x_i in enumerate(X):\n",
    "                condition = y[idx] * np.dot(x_i, self.weights) >= 1\n",
    "                if condition:\n",
    "                    self.weights -= self.lr * (2 * 1/self.n_iter * self.weights)\n",
    "                else:\n",
    "                    self.weights -= self.lr * (2 * 1/self.n_iter * self.weights - np.dot(x_i, y[idx]))\n",
    "\n",
    "    def predict(self, X):\n",
    "        linear_output = np.dot(X, self.weights)\n",
    "        return np.sign(linear_output)\n",
    "\n",
    "    def eval(self, X, Y):\n",
    "        y_pred = self.predict(X)\n",
    "        y_pred = np.where(y_pred == -1, 0, 1)  # Convert back to binary labels\n",
    "        return accuracy_score(Y, y_pred)\n",
    "    \n",
    "\n",
    "    # Function to perform k-fold cross-validation with handcoded models\n",
    "def cross_validate_handcoded_model(X, Y, model, k=5):\n",
    "    kf = KFold(n_splits=k, shuffle=True, random_state=42)\n",
    "    accuracies = []\n",
    "\n",
    "    for train_index, test_index in kf.split(X):\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        Y_train, Y_test = Y[train_index], Y[test_index]\n",
    "        \n",
    "        model.fit(X_train, Y_train)  \n",
    "        accuracy = model.eval(X_test, Y_test)  \n",
    "        accuracies.append(accuracy)\n",
    "\n",
    "    return np.mean(accuracies)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "factsSource = './facts.txt'\n",
    "fakesSource = './fakes.txt'\n",
    "\n",
    "with open(factsSource, 'r') as f:\n",
    "    facts = f.readlines()\n",
    "\n",
    "with open(fakesSource, 'r') as f:\n",
    "    fakes = f.readlines()\n",
    "\n",
    "data = pd.DataFrame({\n",
    "    'content': facts + fakes,\n",
    "    'label': [1] * len(facts) + [0] * len(fakes)  # Label facts as 1, fakes as 0\n",
    "})\n",
    "\n",
    "# Shuffle data\n",
    "data = data.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "\n",
    "# Split the dataset \n",
    "traindf, testdf = train_test_split(data, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the vectorization function\n",
    "Xtrain, Xtest, ytrain, ytest, vectorizer = vectorize_datasets(\n",
    "    train_df=traindf,\n",
    "    test_df=testdf,\n",
    "    vectorizer_type='tfidf',  # Vectorizer Options: 'count' or 'tfidf'\n",
    "    ngram_range=(1, 2),  # Bag-Of-Words Options\n",
    "    remove_stopwords=True,  # Stop Word Removal\n",
    "    apply_lemmatization=True  # Lemmatization\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes 5-fold cross-validation accuracy: 0.957143\n",
      "Logistic Regression 5-fold cross-validation accuracy: 0.961905\n",
      "SVM 5-fold cross-validation accuracy: 0.961905\n"
     ]
    }
   ],
   "source": [
    "# Example usage for Naive Bayes\n",
    "nb_model = NaiveBayes(laplace=1e-10)\n",
    "mean_accuracy_nb = cross_validate_handcoded_model(Xtrain.toarray(), np.array(ytrain), nb_model, k=5)\n",
    "print(f\"Naive Bayes 5-fold cross-validation accuracy: {mean_accuracy_nb:4f}\")\n",
    "\n",
    "# Example usage for Logistic Regression\n",
    "lr_model = LogisticRegressionCustom(lr=5, n_iter=1000)\n",
    "mean_accuracy_lr = cross_validate_handcoded_model(Xtrain.toarray(), np.array(ytrain), lr_model, k=5)\n",
    "print(f\"Logistic Regression 5-fold cross-validation accuracy: {mean_accuracy_lr:4f}\")\n",
    "\n",
    "# Example usage for SVM\n",
    "svm_model = SVMCustom(lr=0.01, n_iter=1000, C=1.0)\n",
    "mean_accuracy_svm = cross_validate_handcoded_model(Xtrain.toarray(), np.array(ytrain), svm_model, k=5)\n",
    "print(f\"SVM 5-fold cross-validation accuracy: {mean_accuracy_svm:4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "USING THE SKLEARN MODULE FOR THE NB, LOGISTIC, AND SVM MODELS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "factsSource = './facts.txt'\n",
    "fakesSource = './fakes.txt'\n",
    "\n",
    "# Read the text files\n",
    "with open(factsSource, 'r') as f:\n",
    "    facts = f.readlines()\n",
    "\n",
    "with open(fakesSource, 'r') as f:\n",
    "    fakes = f.readlines()\n",
    "\n",
    "# Create a DataFrame\n",
    "data = pd.DataFrame({\n",
    "    'content': facts + fakes,\n",
    "    'label': [1] * len(facts) + [0] * len(fakes)  # Label facts as 1, fakes as 0\n",
    "})\n",
    "\n",
    "# Shuffle the data\n",
    "data = data.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "traindf, testdf = train_test_split(data, test_size=0.3, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorizing the data\n",
    "Xtrain, Xtest, ytrain, ytest, vectorizer = vectorize_datasets(\n",
    "    train_df=traindf,\n",
    "    test_df=testdf,\n",
    "    vectorizer_type='count',  # You can switch between 'count' and 'tfidf'\n",
    "    ngram_range=(1, 2),  # Use unigrams and bigrams\n",
    "    remove_stopwords=True,  # Remove stop words\n",
    "    apply_lemmatization=True  # Apply lemmatization\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes Accuracy: 0.9500\n",
      "SVM Accuracy: 0.9524\n",
      "Logistic Regression Accuracy: 0.9548\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# Naive Bayes, SVM, and Logistic Regression using Sklearn\n",
    "def sklearn_classifiers(Xtrain, ytrain):\n",
    "    classifiers = {\n",
    "        \"Naive Bayes\": MultinomialNB(),\n",
    "        \"SVM\": SVC(kernel = \"linear\"),\n",
    "        \"Logistic Regression\": LogisticRegression(max_iter=1000)\n",
    "    }\n",
    "    \n",
    "    for name, clf in classifiers.items():\n",
    "        scores = cross_val_score(clf, Xtrain, ytrain, cv=5)\n",
    "        print(f\"{name} Accuracy: {np.mean(scores):.4f}\")\n",
    "\n",
    "# Running the classifiers\n",
    "sklearn_classifiers(Xtrain, ytrain)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running GridSearchCV for NaiveBayes\n",
      "Best Score for NaiveBayes: 0.9571\n",
      "Best Parameters for NaiveBayes: {'alpha': 0.15789473692631578}\n",
      "\n",
      "Running GridSearchCV for SVM\n",
      "Best Score for SVM: 0.9524\n",
      "Best Parameters for SVM: {'C': 0.05263157904210526, 'gamma': 'scale', 'kernel': 'linear'}\n",
      "\n",
      "Running GridSearchCV for Logistic\n",
      "Best Score for Logistic: 0.9548\n",
      "Best Parameters for Logistic: {'C': 0.8947368421157894, 'max_iter': 100, 'solver': 'lbfgs'}\n",
      "\n",
      "Evaluating NaiveBayes with best hyperparameters\n",
      "Test Accuracy for NaiveBayes: 0.9889\n",
      "\n",
      "Evaluating SVM with best hyperparameters\n",
      "Test Accuracy for SVM: 0.9667\n",
      "\n",
      "Evaluating Logistic with best hyperparameters\n",
      "Test Accuracy for Logistic: 0.9611\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameter tuning\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "def hyperparameter_tuning(Xtrain, ytrain, classifiers):\n",
    "\n",
    "    # Hyperparameter ranges to test \n",
    "    alpha = np.linspace(1e-10, 1, 20)\n",
    "    C = np.linspace(1e-10, 1, 20)   \n",
    "    \n",
    "    # Define hyperparameter grids for each classifier\n",
    "    param_grids = {\n",
    "    \"NaiveBayes\": {\n",
    "        'alpha': alpha  # Laplace smoothing parameter\n",
    "    },\n",
    "    \"SVM\": {\n",
    "        'C': C,  # Regularization parameter\n",
    "        'kernel': ['linear', 'rbf'],  # Linear or RBF kernel\n",
    "        'gamma': ['scale', 'auto']  # Kernel coefficient\n",
    "    },\n",
    "    \"Logistic\": {\n",
    "        'C': C,  # Regularization strength\n",
    "        'solver': ['liblinear', 'lbfgs'],  # Solvers for optimization\n",
    "        'max_iter': [100, 200, 300]  # Maximum number of iterations\n",
    "    }\n",
    "    }\n",
    "\n",
    "\n",
    "    # Test hyperparameter \n",
    "    best_params = {}\n",
    "    for name, clf in classifiers.items():\n",
    "        print(f\"Running GridSearchCV for {name}\")\n",
    "        grid_search = GridSearchCV(clf, param_grids[name], cv=5, scoring='accuracy')\n",
    "        grid_search.fit(Xtrain, ytrain)\n",
    "\n",
    "        # Store the best parameters and scores\n",
    "        best_params[name] = {\n",
    "            'best_score': grid_search.best_score_,\n",
    "            'best_params': grid_search.best_params_\n",
    "        }\n",
    "        print(f\"Best Score for {name}: {grid_search.best_score_:.4f}\")\n",
    "        print(f\"Best Parameters for {name}: {grid_search.best_params_}\\n\")\n",
    "        \n",
    "\n",
    "    return best_params\n",
    "\n",
    "\n",
    "# Retrain models with the best hyperparameters and evaluate on the test set\n",
    "def evaluate_best_models(Xtrain, Xtest, ytrain, ytest, best_params):\n",
    "    for name, clf in classifiers.items():\n",
    "        print(f\"Evaluating {name} with best hyperparameters\")\n",
    "        \n",
    "        # Set the best params\n",
    "        clf.set_params(**best_params[name]['best_params'])\n",
    "        clf.fit(Xtrain, ytrain)\n",
    "        \n",
    "        # Evaluate the model on the test set\n",
    "        test_accuracy = clf.score(Xtest, ytest)\n",
    "        print(f\"Test Accuracy for {name}: {test_accuracy:.4f}\\n\")\n",
    "\n",
    "\n",
    "\n",
    "# Hyperparameter tuning\n",
    "classifiers = {\n",
    "     \"NaiveBayes\": MultinomialNB(),\n",
    "     \"SVM\": SVC(),\n",
    "     \"Logistic\": LogisticRegression()\n",
    " }\n",
    "\n",
    "# Find the best params\n",
    "best_params = hyperparameter_tuning(Xtrain, ytrain, classifiers)\n",
    "# Retrain and eval the models with the best params \n",
    "evaluate_best_models(Xtrain, Xtest, ytrain, ytest, best_params)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comment on the cross validation results (and why we ended up using the count strategy and not tdidf): \n",
    "- Using tfidf strategy yields better accuracy overall when testing for the best hyperparameters but lower accuracies on unseen data (the test set). This may suggest overfitting on the hyperparameters in training \n",
    "- Using count strategy yields lower accuracy when testing for the best hyperparameters but sizeably higher accuracies on evaluating the model using the fitted hyperparameters against unseen data (test set)\n",
    "\n",
    "Also side note  but we see from the models here that it seems like Naive Bayes works the best out of the three. Probably because of the small dataset. \n",
    "-> Maybe test with a bigger dataset some time."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
